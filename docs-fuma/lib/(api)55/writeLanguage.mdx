---
title: Generate language model reply using agent prompts
full: true
---

- ‚ùì **Inputs**: üëÑ Language Intelligence Provider, üîë API Key,  ü§ñ agent
template name, üß† model name and options,
  and üÜé context variables for that agent
- ü§ñ **Agent Instruction Templates**:
[LangHub](https://smith.langchain.com/hub) template or custom: 
  question(query, chat_history), summarize-bullets(article), summarize(article), 
  suggest-followups(chat_history, article), answer-cite-sources(context, chat_history, query),
  query-resolution(chat_history, query), knowledge-graph-nodes(query, article), 
  summary-longtext(summaries)
- üß† **How Language Models Work**: Language models learn from billions of text
examples to 

identify statistical patterns and structures across diverse sources,
converting words into
  high-dimensional vectors‚Äînumerical lists that capture meaning and relationships between concepts. 
  These mathematical representations allow models to understand that "king/queen" share properties 
  and "Paris/France" mirrors "Tokyo/Japan" through their transformer architecture, a neural network 
  backbone that processes information through multiple layers of analysis. The attention mechanism 
  enables the system to dynamically focus on relevant parts of input text when generating each word, 
  maintaining context like humans tracking conversation threads, while calculating probability scores 
  across the entire vocabulary for each word position based on processed context. Rather than retrieving
  stored responses, models create novel text by selecting the most probable words given learned 
  patterns, maintaining coherence across long passages while adapting to specific prompt nuances
    through deep pattern recognition.  
    **Self-Attention**: Each word creates three representations: Query (what it's looking for), Key (what
    it offers), and Value (its actual content). For example, in "The cat sat on the mat," the word "cat" 
    has a Query vector that searches for actions, a Key vector that advertises itself as a subject, and 
    a Value vector containing its semantic meaning as an animal. The attention mechanism calculates how
    much "cat" should focus on other words by comparing its Query with their Keys - finding high 
    similarity with "sat" (the action) - then combines the corresponding Value vectors to create a
      contextualized representation where "cat" now understands it's the one doing the sitting.
    
- üìö **Learning Resources**:
    [LLM Training Example](https://github.com/vtempest/ai-research-agent/blob/master/packages/neural-net/src/train/predict-next-word.js),
    [LangChain ReactAgent Tools](https://medium.com/@terrycho/how-langchain-agent-works-internally-trace-by-using-langsmith-df23766e7fb4),
    [Hugging Face Tutorials](https://huggingface.co/learn), [OpenAI Cookbook](https://cookbook.openai.com),
    [Transformer Overview](https://jalammar.github.io/illustrated-transformer/),
    [Building Transformer Guide](https://www.datacamp.com/tutorial/building-a-transformer-with-py-torch),
    [PyTorch Overview](https://www.learnpytorch.io/pytorch_cheatsheet/)
    
### üëÑ  Language Intelligence Providers (LIPs)

  | üëÑ Provider | ü§ñ Model Families | üìö Docs | üîë Keys | üí∞ Valuation | üí∏ Revenue (2024) | üí≤ Cost (1M Output) |
  | :-- | :-- | :-- | :-- | :-- | :-- | :-- |
  | **XAI** | Grok, Grok Vision | [Docs](https://docs.x.ai/docs#models) | [Keys](https://console.x.ai/) | \$80B | \$100M | \$15.00 |
  | **Groq** | Llama, DeepSeek, Gemini, Mistral | [Docs](https://console.groq.com/docs/overview) | [Keys](https://console.groq.com/keys) | \$2.8B | - | \$0.79 |
  | **Ollama** | llama, mistral, mixtral, vicuna, gemma, qwen, deepseek, openchat, openhermes, codelama, codegemma, llava, minicpm, wizardcoder, wizardmath, meditrion, falcon | [Docs](https://ollama.com/docs) | [Keys](https://ollama.com/settings/keys) | - | \$3.2M | \$0 |
  | **OpenAI** | o1, o1-mini, o4, o4-mini, gpt-4, gpt-4-turbo, gpt-4-omni | [Docs](https://platform.openai.com/docs/overview) | [Keys](https://platform.openai.com/api-keys) | \$300B | \$3.7B | \$8.00 |
  | **Anthropic** | Claude Sonnet, Claude Opus, Claude Haiku | [Docs](https://docs.anthropic.com/en/docs/welcome) | [Keys](https://console.anthropic.com/settings/keys) | \$61.5B | \$1B | \$15.00 |
  | **TogetherAI** | Llama, Mistral, Mixtral, Qwen, Gemma, WizardLM, DBRX, DeepSeek, Hermes, SOLAR, StripedHyena | [Docs](https://docs.together.ai/docs/quickstart) | [Keys](https://api.together.xyz/settings/api-keys) | \$3.3B | \$50M | \$0.90 |
  | **Perplexity** | Sonar, Sonar Deep Research | [Docs](https://docs.perplexity.ai/models/model-cards) | [Keys](https://www.perplexity.ai/account/api/keys) | \$18B | \$20M | \$15.00 |
  | **Cloudflare** | Llama, Gemma, Mistral, Phi, Qwen, DeepSeek, Hermes, SQL Coder, Code Llama | [Docs](https://developers.cloudflare.com/workers-ai/) | [Keys](https://dash.cloudflare.com/profile/api-tokens) | \$62.3B | \$1.67B | \$2.25 |
  | **Google** | Gemini | [Docs](https://cloud.google.com/vertex-ai/generative-ai/docs/learn/models) | [Keys](https://cloud.google.com/vertex-ai/generative-ai/docs/start/express-mode/overview#api-keys) | - | ~$400M | \$10.00 |

![agent_arch_viz](https://i.imgur.com/bailW5n.gif)

![agent_arch_viz2](https://i.imgur.com/uW6E9VJ.gif)


<APIPage document={"openapi.yml"} operations={[{"path":"/agents","method":"post"}]} webhooks={[]} hasHead={false} />